{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8v2LQf5_MZW7"
      },
      "outputs": [],
      "source": [
        "# @title üöÄ 1. Install Dependencies & Setup\n",
        "# This cell installs the necessary libraries to talk to ArangoDB and process the data.\n",
        "# Run this cell first!\n",
        "\n",
        "!pip install python-arango datasets ollama gradio sentence-transformers -q\n",
        "\n",
        "import time\n",
        "from getpass import getpass\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import subprocess\n",
        "import requests\n",
        "import sys\n",
        "import re\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import List, Dict\n",
        "from arango.exceptions import ServerConnectionError, ArangoServerError\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pickle\n",
        "from sentence_transformers import CrossEncoder\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import gradio as gr\n",
        "\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "\n",
        "print(\"‚úÖ Libraries installed.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_and_pull_model(model_name=\"deepseek-r1:8b\"):\n",
        "    \"\"\"\n",
        "    Checks if the model exists in Ollama. If not, pulls it automatically.\n",
        "    \"\"\"\n",
        "    print(f\"üïµÔ∏è [Ollama] Checking for model: {model_name}...\")\n",
        "\n",
        "    # 1. Check list of models\n",
        "    try:\n",
        "        result = subprocess.run([\"ollama\", \"list\"], capture_output=True, text=True)\n",
        "        if model_name in result.stdout:\n",
        "            print(f\"‚úÖ [Ollama] Model '{model_name}' is ready.\")\n",
        "            return\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è [Ollama] Could not check model list: {e}\")\n",
        "\n",
        "    # 2. If missing, pull it\n",
        "    print(f\"‚¨áÔ∏è [Ollama] Model not found. Pulling {model_name} (This takes 2-5 mins)...\")\n",
        "    try:\n",
        "        # We use Popen to stream the output so you don't think it hung\n",
        "        process = subprocess.Popen(\n",
        "            [\"ollama\", \"pull\", model_name],\n",
        "            stdout=subprocess.PIPE,\n",
        "            stderr=subprocess.PIPE\n",
        "        )\n",
        "        while True:\n",
        "            output = process.stderr.readline()\n",
        "            if output == b'' and process.poll() is not None:\n",
        "                break\n",
        "            if output:\n",
        "                # Print progress to console\n",
        "                print(output.decode().strip())\n",
        "\n",
        "        print(f\"‚úÖ [Ollama] Successfully pulled {model_name}!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå [Ollama] Failed to pull model: {e}\")\n",
        "        sys.exit(1) # Stop script if model fails\n",
        "\n",
        "MODEL_NAME = \"deepseek-r1:8b\"\n",
        "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
        "\n",
        "\n",
        "check_and_pull_model()"
      ],
      "metadata": {
        "id": "4zktFyVCj_vW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# --- 3. ROBUST UTILITIES ---\n",
        "\n",
        "class FuzzyEvaluator:\n",
        "    \"\"\"Evaluates answers with logic to handle verbosity and synonyms.\"\"\"\n",
        "\n",
        "    def extract_answer(self, text: str) -> str:\n",
        "        # Strip DeepSeek \"Thinking\" blocks\n",
        "        clean_text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL).lower()\n",
        "        # Look for the last explicit declaration\n",
        "        match = re.search(r'(?:final answer|answer):?\\s*(yes|no|maybe)', clean_text)\n",
        "        if match: return match.group(1)\n",
        "        # Fallback: look for isolated words at end of text\n",
        "        matches = re.findall(r'\\b(yes|no|maybe)\\b', clean_text)\n",
        "        if matches: return matches[-1]\n",
        "        return \"maybe\" # Default safety\n",
        "\n",
        "    def is_correct(self, gt: str, pred: str) -> bool:\n",
        "        gt, pred = gt.lower().strip(), pred.lower().strip()\n",
        "\n",
        "        # 1. Exact Match\n",
        "        if gt == pred: return True\n",
        "\n",
        "        # 2. Starts With (e.g. \"yes, because...\")\n",
        "        if pred.startswith(gt + \" \") or pred.startswith(gt + \",\"): return True\n",
        "\n",
        "        # 3. Synonyms\n",
        "        positive = [\"definitely yes\", \"likely\", \"probable\", \"certainly\"]\n",
        "        negative = [\"unlikely\", \"doubtful\", \"never\"]\n",
        "\n",
        "        if gt == \"yes\" and any(x in pred for x in positive): return True\n",
        "        if gt == \"no\" and any(x in pred for x in negative): return True\n",
        "\n",
        "        return False\n",
        "\n",
        "class ArangoConnectionManager:\n",
        "    \"\"\"Handles the 503 Service Unavailable errors by retrying.\"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.client = ArangoClient(hosts=config[\"hosts\"])\n",
        "        self.db = self._connect_with_retry()\n",
        "\n",
        "    def _connect_with_retry(self, max_retries=5):\n",
        "        for attempt in range(max_retries):\n",
        "            try:\n",
        "                # verify connection\n",
        "                sys_db = self.client.db(\"_system\", username=self.config[\"username\"], password=self.config[\"password\"])\n",
        "                sys_db.version() # Ping\n",
        "\n",
        "                # Connect to actual DB\n",
        "                db = self.client.db(self.config[\"db_name\"], username=self.config[\"username\"], password=self.config[\"password\"])\n",
        "                print(f\"‚úÖ [ArangoDB] Connected successfully.\")\n",
        "                return db\n",
        "            except (ServerConnectionError, ArangoServerError) as e:\n",
        "                wait = (attempt + 1) * 5\n",
        "                print(f\"‚ö†Ô∏è [ArangoDB] Connection failed ({e}). Retrying in {wait}s...\")\n",
        "                time.sleep(wait)\n",
        "\n",
        "        raise ConnectionError(\"Could not connect to ArangoDB after retries.\")"
      ],
      "metadata": {
        "id": "_iTxmLlfNGNB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. THE CACHING FUNCTION (Defined locally)\n",
        "# ==========================================\n",
        "def load_vectors_smartly(db, collection_name, cache_file=\"pubmed_vectors_cache.pkl\"):\n",
        "    \"\"\"\n",
        "    Handles the logic: Check Disk -> If Missing, Download -> Save to Disk.\n",
        "    \"\"\"\n",
        "    # A. Check Disk\n",
        "    if os.path.exists(cache_file):\n",
        "        print(f\"üíæ [Cache] Found local file: {cache_file}\")\n",
        "        try:\n",
        "            with open(cache_file, 'rb') as f:\n",
        "                data = pickle.load(f)\n",
        "            ids = data.get('ids', [])\n",
        "            texts = data.get('texts', [])\n",
        "            embeddings = data.get('embeddings', [])\n",
        "\n",
        "            if len(embeddings) > 0:\n",
        "                print(f\"‚úÖ [Cache] Loaded {len(embeddings)} vectors from disk instantly.\")\n",
        "                return ids, texts, embeddings\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è [Cache] File corrupted ({e}). Re-downloading...\")\n",
        "\n",
        "    # B. Download from Cloud (Only if A failed)\n",
        "    print(f\"‚òÅÔ∏è [Index] Cache missing. Downloading from ArangoDB (This happens only once)...\")\n",
        "\n",
        "    ids, texts, embeddings = [], [], []\n",
        "\n",
        "    # Get Count\n",
        "    try:\n",
        "        count = db.aql.execute(f\"RETURN LENGTH({collection_name})\").next()\n",
        "    except:\n",
        "        count = 200000\n",
        "\n",
        "    # Paged Download\n",
        "    BATCH_SIZE = 5000\n",
        "    offset = 0\n",
        "\n",
        "    with tqdm(total=count, desc=\"Downloading Index\", unit=\"vec\") as pbar:\n",
        "        while True:\n",
        "            aql = f\"\"\"\n",
        "            FOR c IN {collection_name}\n",
        "                FILTER c.embedding != null\n",
        "                LIMIT {offset}, {BATCH_SIZE}\n",
        "                RETURN {{ \"id\": c._id, \"text\": c.text, \"emb\": c.embedding }}\n",
        "            \"\"\"\n",
        "            try:\n",
        "                cursor = db.aql.execute(aql, ttl=3600)\n",
        "                batch_count = 0\n",
        "                for doc in cursor:\n",
        "                    ids.append(doc[\"id\"])\n",
        "                    texts.append(doc[\"text\"])\n",
        "                    embeddings.append(doc[\"emb\"])\n",
        "                    batch_count += 1\n",
        "\n",
        "                pbar.update(batch_count)\n",
        "                offset += batch_count\n",
        "                if batch_count < BATCH_SIZE: break\n",
        "                time.sleep(0.1) # Be gentle on the server\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Error on batch: {e}\")\n",
        "                if \"503\" in str(e): time.sleep(5)\n",
        "                else: break\n",
        "\n",
        "    # C. Save to Disk\n",
        "    embeddings_np = np.array(embeddings)\n",
        "    if len(ids) > 0:\n",
        "        print(f\"üíæ [Cache] Saving {len(ids)} vectors to {cache_file}...\")\n",
        "        with open(cache_file, 'wb') as f:\n",
        "            pickle.dump({'ids': ids, 'texts': texts, 'embeddings': embeddings_np}, f)\n",
        "        print(\"‚úÖ [Cache] Saved.\")\n",
        "\n",
        "    return ids, texts, embeddings_np\n",
        "\n",
        "class RobustGraphRAG:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.client = ArangoClient(hosts=config[\"hosts\"])\n",
        "        self.db = self.client.db(config[\"db_name\"], username=config[\"username\"], password=config[\"password\"])\n",
        "\n",
        "        print(\"‚è≥ [Model] Loading Encoders...\")\n",
        "        self.encoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "        self.reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "\n",
        "        self.chunk_ids, self.chunk_texts, self.chunk_embeddings = load_vectors_smartly(\n",
        "            self.db,\n",
        "            self.config['chunk_col']\n",
        "        )\n",
        "\n",
        "    def retrieve(self, query: str, top_k=3):\n",
        "        if len(self.chunk_embeddings) == 0: return \"No context.\"\n",
        "\n",
        "        # 1. Wider Vector Search (75 candidates)\n",
        "        # We widen this to ensure we catch \"Conclusion\" chunks that might use different wording\n",
        "        query_emb = self.encoder.encode([query])\n",
        "        sims = cosine_similarity(query_emb, self.chunk_embeddings)[0]\n",
        "        top_n_indices = np.argsort(sims)[-75:][::-1]\n",
        "\n",
        "        candidate_pairs = []\n",
        "        for idx in top_n_indices:\n",
        "            candidate_pairs.append((self.chunk_texts[idx], self.chunk_ids[idx]))\n",
        "\n",
        "        # 2. Re-Ranking\n",
        "        cross_inputs = [[query, text] for text, _ in candidate_pairs]\n",
        "        scores = self.reranker.predict(cross_inputs)\n",
        "        ranked_indices = np.argsort(scores)[::-1]\n",
        "\n",
        "        best_chunk_ids = []\n",
        "        for i in range(top_k):\n",
        "            idx = ranked_indices[i]\n",
        "            _, cid = candidate_pairs[idx]\n",
        "            best_chunk_ids.append(cid)\n",
        "\n",
        "        # 3. Graph Expansion (Parent Abstract Reconstruction)\n",
        "        aql = \"\"\"\n",
        "        WITH Papers, Chunks\n",
        "        FOR start_chunk_id IN @ids\n",
        "            LET start_doc = DOCUMENT(start_chunk_id)\n",
        "\n",
        "            // Find Parent Paper\n",
        "            FOR paper IN 1..1 INBOUND start_doc HAS_CONTEXT\n",
        "\n",
        "                // Get ALL chunks (Introduction + Results + Conclusion)\n",
        "                LET full_text_chunks = (\n",
        "                    FOR c IN 1..1 OUTBOUND paper HAS_CONTEXT\n",
        "                    RETURN c.text\n",
        "                )\n",
        "\n",
        "                // Concatenate into a clean abstract\n",
        "                LET full_abstract = CONCAT_SEPARATOR(\" \", full_text_chunks)\n",
        "\n",
        "                RETURN {\n",
        "                    \"title\": paper.title,\n",
        "                    \"abstract\": full_abstract\n",
        "                }\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            cursor = self.db.aql.execute(aql, bind_vars={\"ids\": best_chunk_ids})\n",
        "            context_parts = []\n",
        "            seen_titles = set()\n",
        "\n",
        "            for res in cursor:\n",
        "                title = res.get('title', 'Unknown')\n",
        "                if title in seen_titles: continue\n",
        "                seen_titles.add(title)\n",
        "\n",
        "                # Add \"Study X\" header to help LLM distinguish separate papers\n",
        "                entry = (\n",
        "                    f\"=== STUDY: {title} ===\\n\"\n",
        "                    f\"ABSTRACT: {res.get('abstract')}\\n\"\n",
        "                )\n",
        "                context_parts.append(entry)\n",
        "\n",
        "            return \"\\n\".join(context_parts)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Graph Error ({e}).\")\n",
        "            fallback_texts = []\n",
        "            for i in range(top_k):\n",
        "                idx = ranked_indices[i]\n",
        "                t, _ = candidate_pairs[idx]\n",
        "                fallback_texts.append(f\"Excerpt: {t}\")\n",
        "            return \"\\n\".join(fallback_texts)\n",
        "\n",
        "    def _heuristic_override(self, response_text):\n",
        "        \"\"\"\n",
        "        Python Safety Net: Catches 'Maybe' and flips it if strong keywords exist.\n",
        "        \"\"\"\n",
        "        clean_text = response_text.lower()\n",
        "\n",
        "        # 1. Extract the explicit answer\n",
        "        match = re.search(r'(?:final answer|answer):?\\s*(yes|no|maybe)', clean_text)\n",
        "        pred = match.group(1) if match else \"maybe\"\n",
        "\n",
        "        # 2. If prediction is YES or NO, trust the model.\n",
        "        if pred in [\"yes\", \"no\"]:\n",
        "            return pred\n",
        "\n",
        "        # 3. If prediction is MAYBE, check the REASONING for \"Soft Signals\"\n",
        "        # Positive Signals\n",
        "        soft_yes = [\"suggests\", \"indicates\", \"significant\", \"associated with\", \"effective\", \"improved\"]\n",
        "        for word in soft_yes:\n",
        "            if word in clean_text:\n",
        "                return \"yes\"\n",
        "\n",
        "        # Negative Signals\n",
        "        soft_no = [\"no significant\", \"did not\", \"unrelated\", \"ineffective\", \"no difference\"]\n",
        "        for word in soft_no:\n",
        "            if word in clean_text:\n",
        "                return \"no\"\n",
        "\n",
        "        return \"maybe\"\n",
        "\n",
        "    def query_ollama(self, prompt: str):\n",
        "        # The \"Calibration\" Prompt\n",
        "        # We align the model with PubMedQA's specific annotation style.\n",
        "\n",
        "        system_msg = \"\"\"\n",
        "        You are a PubMedQA annotator.\n",
        "        Your task is to classify the answer as 'yes', 'no', or 'maybe' based on the Study Abstract.\n",
        "\n",
        "        ANNOTATION GUIDELINES (CRITICAL):\n",
        "        1. If the study suggests a positive outcome, even if \"further study is needed\", the answer is YES.\n",
        "        2. If the study finds a correlation or association, the answer is YES.\n",
        "        3. If the study finds \"no significant difference\", the answer is NO.\n",
        "        4. ONLY use MAYBE if the abstract explicitly states \"results were inconclusive\" or provides zero data.\n",
        "\n",
        "        Format:\n",
        "        Final Answer: [yes/no/maybe]\n",
        "        \"\"\"\n",
        "\n",
        "        full_prompt = f\"{system_msg}\\n\\nContext:\\n{prompt}\"\n",
        "\n",
        "        url = \"http://localhost:11434/api/chat\"\n",
        "        payload = {\n",
        "            \"model\": \"deepseek-r1:8b\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            \"stream\": False,\n",
        "            \"options\": {\n",
        "                \"temperature\": 0.0,\n",
        "                \"num_ctx\": 4096\n",
        "            }\n",
        "        }\n",
        "        try:\n",
        "            res = requests.post(url, json=payload, timeout=300)\n",
        "            if res.status_code == 200:\n",
        "                raw_response = res.json()['message']['content']\n",
        "\n",
        "                # --- APPLY THE PYTHON SAFETY NET ---\n",
        "                final_decision = self._heuristic_override(raw_response)\n",
        "\n",
        "                # Return a format that your evaluator can parse\n",
        "                return f\"{raw_response}\\n\\n[Heuristic Override Result]: Final Answer: {final_decision}\"\n",
        "\n",
        "            return f\"Error {res.status_code}\"\n",
        "        except Exception as e:\n",
        "            return f\"Exception: {e}\"\n",
        "\n",
        "\n",
        "\n",
        "    def generate_chat_response(self, message, context):\n",
        "        \"\"\"\n",
        "        A specific prompt for the Chat UI (Conversational, not Yes/No).\n",
        "        \"\"\"\n",
        "        system_msg = \"\"\"\n",
        "        You are a Helpful Medical AI Assistant.\n",
        "        Use the provided Research Abstracts to answer the user's question accurately.\n",
        "\n",
        "        Guidelines:\n",
        "        1. Base your answer ONLY on the context provided.\n",
        "        2. Cite the specific study titles when making claims (e.g., \"According to the study on X...\").\n",
        "        3. If the studies are conflicting, explain the conflict.\n",
        "        4. If the answer is not in the context, admit you don't have evidence but give your opinion.\n",
        "        \"\"\"\n",
        "\n",
        "        full_prompt = f\"{system_msg}\\n\\nContext:\\n{context}\\n\\nUser Question: {message}\"\n",
        "\n",
        "        url = \"http://localhost:11434/api/chat\"\n",
        "        payload = {\n",
        "            \"model\": \"deepseek-r1:8b\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": full_prompt}],\n",
        "            \"stream\": False,\n",
        "            \"options\": {\"temperature\": 0.3, \"num_ctx\": 4096} # Slight creativity allowed\n",
        "        }\n",
        "        try:\n",
        "            res = requests.post(url, json=payload, timeout=300)\n",
        "            if res.status_code == 200:\n",
        "                return res.json()['message']['content']\n",
        "            return \"Error: Could not communicate with model.\"\n",
        "        except Exception as e:\n",
        "            return f\"Error: {e}\"\n",
        "\n",
        "    # --- THE UI LAUNCHER ---\n",
        "    def launch_gradio_ui(self):\n",
        "        print(\"\\nüöÄ Launching Gradio UI...\")\n",
        "\n",
        "        def chat_logic(message, history):\n",
        "            # 1. Retrieve Context\n",
        "            print(f\"üîé Retrieving for: {message}...\")\n",
        "            retrieved_context = self.retrieve(message)\n",
        "\n",
        "            # 2. Generate Answer\n",
        "            print(f\"ü§ñ Generating Answer...\")\n",
        "            response = self.generate_chat_response(message, retrieved_context)\n",
        "\n",
        "            # 3. Optional: Append Sources to the bottom of the answer\n",
        "            final_output = f\"{response}\\n\\n___\\n**Sources Retrieved:**\\n\"\n",
        "\n",
        "            # Simple regex to extract titles for display\n",
        "            titles = re.findall(r\"=== STUDY: (.*?) ===\", retrieved_context)\n",
        "            for t in titles:\n",
        "                final_output += f\"- *{t}*\\n\"\n",
        "\n",
        "            return final_output\n",
        "\n",
        "        # Create the Interface\n",
        "        demo = gr.ChatInterface(\n",
        "            fn=chat_logic,\n",
        "            title=\"üß¨ PubMed GraphRAG Assistant\",\n",
        "            description=\"Ask detailed medical questions. I will retrieve full abstracts from the Knowledge Graph to answer you.\",\n",
        "            examples=[\n",
        "                \"Do preoperative statins reduce atrial fibrillation?\",\n",
        "                \"Is obesity a risk factor for cirrhosis-related death or hospitalization?\",\n",
        "                \"Does high-dose aspirin prevent cardiovascular events?\"\n",
        "            ],\n",
        "            theme=\"soft\"\n",
        "        )\n",
        "\n",
        "        demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "djEmPjhjNLej"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedEvaluator:\n",
        "    def __init__(self):\n",
        "        self.y_true = []\n",
        "        self.y_pred = []\n",
        "        self.start_time = None\n",
        "        self.end_time = None\n",
        "\n",
        "    def start(self):\n",
        "        \"\"\"Starts the stopwatch.\"\"\"\n",
        "        self.start_time = time.time()\n",
        "        print(\"‚è±Ô∏è Evaluation Timer Started...\")\n",
        "\n",
        "    def stop(self):\n",
        "        \"\"\"Stops the stopwatch.\"\"\"\n",
        "        self.end_time = time.time()\n",
        "\n",
        "    def record(self, gt, pred):\n",
        "        \"\"\"Records a single prediction pair.\"\"\"\n",
        "        # Normalize to ensure clean metrics\n",
        "        clean_gt = gt.lower().strip()\n",
        "        clean_pred = pred.lower().strip()\n",
        "\n",
        "        # Safety: If model output garbage, classify as 'maybe'\n",
        "        if clean_pred not in ['yes', 'no', 'maybe']:\n",
        "            clean_pred = 'maybe'\n",
        "\n",
        "        self.y_true.append(clean_gt)\n",
        "        self.y_pred.append(clean_pred)\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"Calculates and visualizes all requested metrics.\"\"\"\n",
        "        if not self.y_true:\n",
        "            print(\"‚ö†Ô∏è No data to report.\")\n",
        "            return\n",
        "\n",
        "        # 1. Total Time\n",
        "        total_seconds = self.end_time - self.start_time\n",
        "        avg_per_sample = total_seconds / len(self.y_true)\n",
        "\n",
        "        # 2. Accuracy\n",
        "        acc = accuracy_score(self.y_true, self.y_pred) * 100\n",
        "\n",
        "        print(\"\\n\" + \"=\"*40)\n",
        "        print(f\"üìä FINAL EVALUATION REPORT\")\n",
        "        print(\"=\"*40)\n",
        "        print(f\"‚è±Ô∏è Total Time:     {total_seconds:.2f} seconds\")\n",
        "        print(f\"‚ö° Avg Latency:    {avg_per_sample:.2f} seconds/query\")\n",
        "        print(f\"üéØ Final Accuracy: {acc:.2f}%\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # 3. Prediction Summary (Counts)\n",
        "        df = pd.DataFrame({'Ground Truth': self.y_true, 'Prediction': self.y_pred})\n",
        "        print(\"\\nüìã Prediction Distribution:\")\n",
        "        print(df['Prediction'].value_counts())\n",
        "\n",
        "        # 4. Classification Report\n",
        "        print(\"\\nüìà Detailed Classification Report:\")\n",
        "        # We specify labels to ensure all classes show up even if count is 0\n",
        "        labels = ['yes', 'no', 'maybe']\n",
        "        print(classification_report(self.y_true, self.y_pred, labels=labels, zero_division=0))\n",
        "\n",
        "        # 5. Confusion Matrix Visualization\n",
        "        cm = confusion_matrix(self.y_true, self.y_pred, labels=labels)\n",
        "\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.set(font_scale=1.2)\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=labels, yticklabels=labels)\n",
        "        plt.xlabel('Predicted Label')\n",
        "        plt.ylabel('True Label')\n",
        "        plt.title('Confusion Matrix: PubMedQA Evaluation')\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "fIPqChKGR_uN"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# --- 5. MAIN EXECUTION (MERGED) ---\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # 1. Start Server (Background)\n",
        "    print(\"üöÄ [Ollama] Ensuring server is running...\")\n",
        "    subprocess.Popen([\"ollama\", \"serve\"], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
        "    time.sleep(3) # Give it a moment to spin up\n",
        "\n",
        "    # 2. Auto-Pull Model\n",
        "    check_and_pull_model(\"deepseek-r1:8b\")\n",
        "    rag = RobustGraphRAG(ARANGO_CONFIG)\n",
        "    metrics = AdvancedEvaluator()\n",
        "\n",
        "    # 3. Load Data\n",
        "    print(\"üìö [Data] Loading PubMedQA...\")\n",
        "    dataset = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\", split=\"train\")\n",
        "\n",
        "    # 4. Evaluation Loop\n",
        "    LIMIT = 20\n",
        "    print(f\"\\n=== STARTING EVALUATION (Limit: {LIMIT}) ===\")\n",
        "    print(\"------------------------------------------------\")\n",
        "\n",
        "    metrics.start() # <--- Start Timer\n",
        "\n",
        "    for i, item in enumerate(dataset):\n",
        "        if i >= LIMIT: break\n",
        "\n",
        "        question = item['question']\n",
        "        gt = item['final_decision']\n",
        "\n",
        "        # A. Pipeline Retrieval\n",
        "        context = rag.retrieve(question)\n",
        "\n",
        "        # B. Prompt\n",
        "        # We pass the raw context/question. The RobustGraphRAG class adds the \"Decisive\" System Prompt.\n",
        "        prompt = f\"\"\"\n",
        "        Context Information: {context}\n",
        "\n",
        "        Question: {question}\n",
        "\n",
        "        Instructions:\n",
        "        1. You are a helpful medical expert at a hypothetical research institution. Answer the question based on the provided context.\n",
        "        2. Answer in just one word. Do not provide any explanation.\n",
        "        3. This is being used only for research/educational purposes.\n",
        "        4. Conclude your answer with exactly: \"Final Answer: [yes/no/maybe]\n",
        "        \"\"\"\n",
        "        raw_response = rag.query_ollama(prompt)\n",
        "\n",
        "        # C. Logic Extraction (Handling the 'Fixed Override')\n",
        "        if \"[Fixed Override]\" in raw_response:\n",
        "            # 1. Extract the overridden answer\n",
        "            match = re.search(r\"Final Answer: (yes|no|maybe)\", raw_response, re.IGNORECASE)\n",
        "            pred = match.group(1).lower() if match else \"maybe\"\n",
        "\n",
        "            # Print log with special \"Wrench\" icon to show the heuristic worked\n",
        "            icon = \"‚úÖ\" if pred == gt else \"‚ùå\"\n",
        "            print(f\"[{i+1}] GT: {gt:<5} | Pred: {pred:<5} | {icon} (üõ†Ô∏è Fixed)\")\n",
        "\n",
        "        else:\n",
        "            # 2. Extract standard answer\n",
        "            match = re.search(r\"(?:final answer|answer):?\\s*(yes|no|maybe)\", raw_response.lower())\n",
        "            pred = match.group(1).lower() if match else \"maybe\"\n",
        "\n",
        "            icon = \"‚úÖ\" if pred == gt else \"‚ùå\"\n",
        "            print(f\"[{i+1}] GT: {gt:<5} | Pred: {pred:<5} | {icon}\")\n",
        "\n",
        "        # D. Record Data point for the Graphs\n",
        "        metrics.record(gt, pred)\n",
        "\n",
        "    # 5. Finalize & Visualize\n",
        "    metrics.stop() # <--- Stop Timer\n",
        "    metrics.generate_report() # <--- Plots Confusion Matrix"
      ],
      "metadata": {
        "id": "VBfaqOFQDxtR",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch UI\n",
        "rag.launch_gradio_ui()"
      ],
      "metadata": {
        "id": "-LJUiQ1CR9Nm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IYaeOxtIBp_p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}