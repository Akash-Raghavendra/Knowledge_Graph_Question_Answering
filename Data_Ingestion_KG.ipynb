{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install python-arango sentence-transformers datasets tqdm"
      ],
      "metadata": {
        "id": "rUGLgHO2I-_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from arango import ArangoClient\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Nyu1zWSUJKbO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ARANGO_CONFIG = {\n",
        "    \"hosts\": \"https://bfc25a0e3c74.arangodb.cloud:8529\",\n",
        "    \"username\": \"root\",\n",
        "    \"password\": \"VnicTWKeXaDasFNfmCfU\",\n",
        "    \"db_name\": \"pubmed_graph\",\n",
        "    \"chunk_col\": \"Chunks\",\n",
        "    \"context_edge\": \"HAS_CONTEXT\",\n",
        "    \"mention_edge\": \"MENTIONS\"\n",
        "}"
      ],
      "metadata": {
        "id": "KAdtBe5TG5E5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dW9J5OorEnsz"
      },
      "outputs": [],
      "source": [
        "# @title ðŸš€ GraphRAG Builder (Fixed & Complete)\n",
        "# This script installs dependencies, connects to ArangoDB, sets up the schema,\n",
        "# and ingests the PubMedQA dataset into the graph.\n",
        "\n",
        "# --- MANUAL CONFIGURATION ---\n",
        "# Paste your details directly here to avoid input errors:\n",
        "\n",
        "# 1. The URL must start with https:// and usually ends with :8529\n",
        "ARANGO_URL = \"https://bfc25a0e3c74.arangodb.cloud:8529\"\n",
        "\n",
        "# 2. The Username is almost always 'root'\n",
        "ARANGO_USER = \"root\"\n",
        "\n",
        "# 3. Paste the password you copied from the 'Users' tab\n",
        "ARANGO_PASS = \"VnicTWKeXaDasFNfmCfU\"\n",
        "\n",
        "# Database Name\n",
        "DB_NAME = \"pubmed_graph\"\n",
        "\n",
        "# --- CONNECT ---\n",
        "print(f\"Connecting to {ARANGO_URL}...\")\n",
        "client = ArangoClient(hosts=ARANGO_URL)\n",
        "sys_db = client.db(\"_system\", username=ARANGO_USER, password=ARANGO_PASS)\n",
        "\n",
        "# Create/Connect to specific database\n",
        "if not sys_db.has_database(DB_NAME):\n",
        "    sys_db.create_database(DB_NAME)\n",
        "    print(f\"Created database: {DB_NAME}\")\n",
        "else:\n",
        "    print(f\"Using existing database: {DB_NAME}\")\n",
        "\n",
        "db = client.db(DB_NAME, username=ARANGO_USER, password=ARANGO_PASS)\n",
        "print(\"âœ… Connected Successfully!\")\n",
        "\n",
        "# --- SCHEMA SETUP ---\n",
        "print(\"\\nCreating Graph Schema...\")\n",
        "\n",
        "# 1. Define Node Collections\n",
        "node_collections = [\"Papers\", \"Chunks\", \"Concepts\"]\n",
        "for col in node_collections:\n",
        "    if not db.has_collection(col):\n",
        "        db.create_collection(col)\n",
        "        print(f\" - Created Node Collection: {col}\")\n",
        "\n",
        "# 2. Define Edge Collections\n",
        "edge_collections = [\"HAS_CONTEXT\", \"MENTIONS\"]\n",
        "for col in edge_collections:\n",
        "    if not db.has_collection(col):\n",
        "        db.create_collection(col, edge=True)\n",
        "        print(f\" - Created Edge Collection: {col}\")\n",
        "\n",
        "# 3. Create ArangoSearch View (Fallback for Vector Search)\n",
        "# FIXED: The 'vector' index type is experimental in your version.\n",
        "# We use an ArangoSearch View instead, which is robust and works on all versions.\n",
        "view_name = \"pubmed_view\"\n",
        "\n",
        "# FIXED: Use db.views() list comprehension to check existence instead of .has_view()\n",
        "existing_views = [v[\"name\"] for v in db.views()]\n",
        "\n",
        "if view_name not in existing_views:\n",
        "    # FIXED: Use dedicated method 'create_arangosearch_view' to avoid TypeError on 'type' arg\n",
        "    db.create_arangosearch_view(\n",
        "        name=view_name,\n",
        "        properties={\n",
        "            \"links\": {\n",
        "                \"Chunks\": {\n",
        "                    \"fields\": {\n",
        "                        \"embedding\": {\n",
        "                            \"analyzers\": [\"identity\"]  # Needed for vector operations\n",
        "                        },\n",
        "                        \"text\": {\n",
        "                            \"analyzers\": [\"text_en\"]   # Useful for keyword search\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    )\n",
        "    print(f\" - Created ArangoSearch View: {view_name}\")\n",
        "else:\n",
        "    print(f\" - ArangoSearch View '{view_name}' already exists.\")\n",
        "\n",
        "print(\"\\nâœ… Database Configured Successfully!\")\n",
        "\n",
        "# --- LOAD DATA & MODEL ---\n",
        "print(\"\\nLoading Embedding Model & Dataset...\")\n",
        "\n",
        "# Load Model (Runs on GPU if available in Colab)\n",
        "# We use all-MiniLM-L6-v2 for speed and good performance\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Load Dataset (Standard download to avoid 429 Rate Limits)\n",
        "# REMOVED: streaming=True to prevent \"Too Many Requests\" error\n",
        "ds = load_dataset(\"qiaojin/PubMedQA\", \"pqa_unlabeled\", split=\"train\")\n",
        "\n",
        "print(\"âœ… Model and Data ready.\")\n",
        "\n",
        "# --- PROCESSING LOOP ---\n",
        "# This loop processes papers, chunks them, embeds them, and inserts into ArangoDB.\n",
        "\n",
        "BATCH_SIZE = 50  # Number of papers to process before sending to DB (smaller batch for safety)\n",
        "LIMIT_PAPERS = None # Limit for this run to ensure it finishes quickly (Set to None for full dataset)\n",
        "\n",
        "papers_batch = []\n",
        "chunks_batch = []\n",
        "concepts_batch = []\n",
        "edges_batch = []\n",
        "\n",
        "print(f\"\\nðŸš€ Starting Ingestion (Limit: {LIMIT_PAPERS} papers)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "count = 0\n",
        "\n",
        "for row in tqdm(ds, total=LIMIT_PAPERS):\n",
        "    if LIMIT_PAPERS and count >= LIMIT_PAPERS:\n",
        "        break\n",
        "\n",
        "    pubid = row['pubid']\n",
        "    question = row['question']\n",
        "    long_answer = row['long_answer']\n",
        "\n",
        "    # 1. Prepare Paper Node\n",
        "    paper_key = str(pubid)\n",
        "    papers_batch.append({\n",
        "        \"_key\": paper_key,\n",
        "        \"title\": question,\n",
        "        \"answer\": long_answer\n",
        "    })\n",
        "\n",
        "    # 2. Process Concepts (MeSH Terms)\n",
        "    mesh_terms = row.get('context', {}).get('meshes', [])\n",
        "    for mesh in mesh_terms:\n",
        "        # Sanitize key (Arango keys cannot contain spaces/special chars easily, so we hash or simplify)\n",
        "        # Here we just remove non-alphanumeric for simplicity\n",
        "        mesh_key = \"\".join(x for x in mesh if x.isalnum())\n",
        "        if not mesh_key: continue\n",
        "\n",
        "        # Add Concept Node\n",
        "        concepts_batch.append({\n",
        "            \"_key\": mesh_key,\n",
        "            \"name\": mesh\n",
        "        })\n",
        "\n",
        "        # Link Paper -> Concept\n",
        "        edges_batch.append({\n",
        "            \"_collection\": \"MENTIONS\",\n",
        "            \"_from\": f\"Papers/{paper_key}\",\n",
        "            \"_to\": f\"Concepts/{mesh_key}\"\n",
        "        })\n",
        "\n",
        "    # 3. Process Contexts (Chunks)\n",
        "    contexts = row.get('context', {}).get('contexts', [])\n",
        "    labels = row.get('context', {}).get('labels', [])\n",
        "\n",
        "    if contexts:\n",
        "        # Embed all chunks for this paper at once\n",
        "        embeddings = model.encode(contexts)\n",
        "\n",
        "        for idx, (text, emb) in enumerate(zip(contexts, embeddings)):\n",
        "            chunk_key = f\"{paper_key}_{idx}\"\n",
        "\n",
        "            # Add Chunk Node\n",
        "            chunks_batch.append({\n",
        "                \"_key\": chunk_key,\n",
        "                \"text\": text,\n",
        "                \"label\": labels[idx] if idx < len(labels) else \"context\",\n",
        "                \"embedding\": emb.tolist() # Convert numpy array to list for JSON\n",
        "            })\n",
        "\n",
        "            # Link Paper -> Chunk\n",
        "            edges_batch.append({\n",
        "                \"_collection\": \"HAS_CONTEXT\",\n",
        "                \"_from\": f\"Papers/{paper_key}\",\n",
        "                \"_to\": f\"Chunks/{chunk_key}\"\n",
        "            })\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    # --- BATCH INSERTION ---\n",
        "    if count % BATCH_SIZE == 0:\n",
        "        # Insert Papers\n",
        "        if papers_batch:\n",
        "            db.collection(\"Papers\").import_bulk(papers_batch, on_duplicate=\"ignore\")\n",
        "        # Insert Concepts\n",
        "        if concepts_batch:\n",
        "            db.collection(\"Concepts\").import_bulk(concepts_batch, on_duplicate=\"ignore\")\n",
        "        # Insert Chunks\n",
        "        if chunks_batch:\n",
        "            db.collection(\"Chunks\").import_bulk(chunks_batch, on_duplicate=\"ignore\")\n",
        "\n",
        "        # Insert Edges (Must split by collection type for import_bulk)\n",
        "        mentions = [e for e in edges_batch if e[\"_collection\"] == \"MENTIONS\"]\n",
        "        contexts = [e for e in edges_batch if e[\"_collection\"] == \"HAS_CONTEXT\"]\n",
        "\n",
        "        if mentions:\n",
        "            db.collection(\"MENTIONS\").import_bulk(mentions, on_duplicate=\"ignore\")\n",
        "        if contexts:\n",
        "            db.collection(\"HAS_CONTEXT\").import_bulk(contexts, on_duplicate=\"ignore\")\n",
        "\n",
        "        # Reset batches\n",
        "        papers_batch = []\n",
        "        chunks_batch = []\n",
        "        concepts_batch = []\n",
        "        edges_batch = []\n",
        "\n",
        "# Final flush for remaining data\n",
        "if papers_batch: db.collection(\"Papers\").import_bulk(papers_batch, on_duplicate=\"ignore\")\n",
        "if concepts_batch: db.collection(\"Concepts\").import_bulk(concepts_batch, on_duplicate=\"ignore\")\n",
        "if chunks_batch: db.collection(\"Chunks\").import_bulk(chunks_batch, on_duplicate=\"ignore\")\n",
        "\n",
        "mentions = [e for e in edges_batch if e[\"_collection\"] == \"MENTIONS\"]\n",
        "contexts = [e for e in edges_batch if e[\"_collection\"] == \"HAS_CONTEXT\"]\n",
        "if mentions: db.collection(\"MENTIONS\").import_bulk(mentions, on_duplicate=\"ignore\")\n",
        "if contexts: db.collection(\"HAS_CONTEXT\").import_bulk(contexts, on_duplicate=\"ignore\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nðŸŽ‰ Finished! Processed {count} papers in {end_time - start_time:.2f} seconds.\")\n",
        "print(f\"Go to your ArangoDB Dashboard to see the 'pubmed_graph' database.\")\n",
        "print(f\"IMPORTANT: Use 'FOR doc IN pubmed_view' in your AQL queries!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title âž• Add PubMedQA \"Labeled\" Subset\n",
        "# This script adds the 1,000 labeled papers to your existing graph.\n",
        "\n",
        "\n",
        "# --- MANUAL CONFIGURATION ---\n",
        "# 1. The URL must start with https:// and usually ends with :8529\n",
        "ARANGO_URL = \"https://bfc25a0e3c74.arangodb.cloud:8529\"\n",
        "# 2. The Username\n",
        "ARANGO_USER = \"root\"\n",
        "# 3. Paste the password you copied from the 'Users' tab\n",
        "ARANGO_PASS = \"VnicTWKeXaDasFNfmCfU\"\n",
        "# Database Name\n",
        "DB_NAME = \"pubmed_graph\"\n",
        "\n",
        "# --- CONNECT ---\n",
        "print(f\"Connecting to {ARANGO_URL}...\")\n",
        "client = ArangoClient(hosts=ARANGO_URL)\n",
        "db = client.db(DB_NAME, username=ARANGO_USER, password=ARANGO_PASS)\n",
        "print(\"âœ… Connected to 'pubmed_graph'!\")\n",
        "\n",
        "# --- LOAD DATA & MODEL ---\n",
        "print(\"\\nLoading 'pqa_labeled' dataset...\")\n",
        "\n",
        "# Load the LABELED subset this time\n",
        "ds_labeled = load_dataset(\"qiaojin/PubMedQA\", \"pqa_labeled\", split=\"train\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(f\"âœ… Loaded {len(ds_labeled)} labeled papers.\")\n",
        "\n",
        "# --- PROCESSING LOOP ---\n",
        "BATCH_SIZE = 50\n",
        "papers_batch = []\n",
        "chunks_batch = []\n",
        "concepts_batch = []\n",
        "edges_batch = []\n",
        "\n",
        "print(\"\\nðŸš€ Starting Ingestion of Labeled Data...\")\n",
        "start_time = time.time()\n",
        "count = 0\n",
        "\n",
        "for row in tqdm(ds_labeled):\n",
        "    pubid = row['pubid']\n",
        "    question = row['question']\n",
        "    long_answer = row['long_answer']\n",
        "    final_decision = row.get('final_decision', None) # Unique to labeled set\n",
        "\n",
        "    # 1. Prepare Paper Node (With extra 'final_decision' field)\n",
        "    paper_key = str(pubid)\n",
        "    papers_batch.append({\n",
        "        \"_key\": paper_key,\n",
        "        \"title\": question,\n",
        "        \"answer\": long_answer,\n",
        "        \"decision\": final_decision,  # Store 'yes', 'no', or 'maybe'\n",
        "        \"dataset\": \"labeled\"         # Tag it so we know source\n",
        "    })\n",
        "\n",
        "    # 2. Process Concepts (MeSH Terms)\n",
        "    mesh_terms = row.get('context', {}).get('meshes', [])\n",
        "    for mesh in mesh_terms:\n",
        "        mesh_key = \"\".join(x for x in mesh if x.isalnum())\n",
        "        if not mesh_key: continue\n",
        "\n",
        "        concepts_batch.append({\n",
        "            \"_key\": mesh_key,\n",
        "            \"name\": mesh\n",
        "        })\n",
        "        edges_batch.append({\n",
        "            \"_collection\": \"MENTIONS\",\n",
        "            \"_from\": f\"Papers/{paper_key}\",\n",
        "            \"_to\": f\"Concepts/{mesh_key}\"\n",
        "        })\n",
        "\n",
        "    # 3. Process Contexts (Chunks)\n",
        "    contexts = row.get('context', {}).get('contexts', [])\n",
        "    labels = row.get('context', {}).get('labels', [])\n",
        "\n",
        "    if contexts:\n",
        "        embeddings = model.encode(contexts)\n",
        "        for idx, (text, emb) in enumerate(zip(contexts, embeddings)):\n",
        "            chunk_key = f\"{paper_key}_{idx}\"\n",
        "            chunks_batch.append({\n",
        "                \"_key\": chunk_key,\n",
        "                \"text\": text,\n",
        "                \"label\": labels[idx] if idx < len(labels) else \"context\",\n",
        "                \"embedding\": emb.tolist()\n",
        "            })\n",
        "            edges_batch.append({\n",
        "                \"_collection\": \"HAS_CONTEXT\",\n",
        "                \"_from\": f\"Papers/{paper_key}\",\n",
        "                \"_to\": f\"Chunks/{chunk_key}\"\n",
        "            })\n",
        "\n",
        "    count += 1\n",
        "\n",
        "    # --- BATCH INSERTION ---\n",
        "    if count % BATCH_SIZE == 0:\n",
        "        if papers_batch: db.collection(\"Papers\").import_bulk(papers_batch, on_duplicate=\"update\") # Update if exists\n",
        "        if concepts_batch: db.collection(\"Concepts\").import_bulk(concepts_batch, on_duplicate=\"ignore\")\n",
        "        if chunks_batch: db.collection(\"Chunks\").import_bulk(chunks_batch, on_duplicate=\"ignore\")\n",
        "\n",
        "        mentions = [e for e in edges_batch if e[\"_collection\"] == \"MENTIONS\"]\n",
        "        contexts = [e for e in edges_batch if e[\"_collection\"] == \"HAS_CONTEXT\"]\n",
        "        if mentions: db.collection(\"MENTIONS\").import_bulk(mentions, on_duplicate=\"ignore\")\n",
        "        if contexts: db.collection(\"HAS_CONTEXT\").import_bulk(contexts, on_duplicate=\"ignore\")\n",
        "\n",
        "        papers_batch = []\n",
        "        chunks_batch = []\n",
        "        concepts_batch = []\n",
        "        edges_batch = []\n",
        "\n",
        "# Final Flush\n",
        "if papers_batch: db.collection(\"Papers\").import_bulk(papers_batch, on_duplicate=\"update\")\n",
        "if concepts_batch: db.collection(\"Concepts\").import_bulk(concepts_batch, on_duplicate=\"ignore\")\n",
        "if chunks_batch: db.collection(\"Chunks\").import_bulk(chunks_batch, on_duplicate=\"ignore\")\n",
        "mentions = [e for e in edges_batch if e[\"_collection\"] == \"MENTIONS\"]\n",
        "contexts = [e for e in edges_batch if e[\"_collection\"] == \"HAS_CONTEXT\"]\n",
        "if mentions: db.collection(\"MENTIONS\").import_bulk(mentions, on_duplicate=\"ignore\")\n",
        "if contexts: db.collection(\"HAS_CONTEXT\").import_bulk(contexts, on_duplicate=\"ignore\")\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"\\nðŸŽ‰ Added {count} labeled papers in {end_time - start_time:.2f} seconds.\")"
      ],
      "metadata": {
        "id": "q_dQy8L2Ew8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ðŸ§¹ Remove \"decision\" and \"dataset\" columns\n",
        "# This script iterates through Papers and deletes the specific attributes.\n",
        "\n",
        "# 1. Define the AQL Query\n",
        "# We filter for papers that actually have these fields to save processing time.\n",
        "# Setting them to 'null' with 'keepNull: false' deletes the attribute entirely.\n",
        "aql_clean_columns = \"\"\"\n",
        "FOR p IN Papers\n",
        "  FILTER HAS(p, \"decision\") OR HAS(p, \"dataset\")\n",
        "\n",
        "  UPDATE p WITH {\n",
        "    decision: null,\n",
        "    dataset: null\n",
        "  } IN Papers\n",
        "  OPTIONS { keepNull: false }\n",
        "\"\"\"\n",
        "\n",
        "# 2. Execute\n",
        "print(\"Removing 'decision' and 'dataset' columns...\")\n",
        "cursor = db.aql.execute(aql_clean_columns)\n",
        "\n",
        "# 3. Verify\n",
        "# Let's count if any remain\n",
        "verification_query = \"\"\"\n",
        "FOR p IN Papers\n",
        "  FILTER HAS(p, \"decision\")\n",
        "  COLLECT WITH COUNT INTO count\n",
        "  RETURN count\n",
        "\"\"\"\n",
        "count = list(db.aql.execute(verification_query))[0]\n",
        "\n",
        "if count == 0:\n",
        "    print(\"âœ… Success! Columns removed. All papers now have a uniform schema.\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Something went wrong. {count} papers still have the decision column.\")"
      ],
      "metadata": {
        "id": "olhs2y-2Eyww"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}